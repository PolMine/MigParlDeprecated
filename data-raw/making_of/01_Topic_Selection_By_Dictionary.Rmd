---
title: "01_Topic Selection by Dictionary"
author: "Christoph Leonhardt"
date: "03 November 2019"
output: html_document
---

```{r 01_load_libraries, eval = TRUE, echo = FALSE}
library(magrittr)
```

# 1. Creating a meaningful subset of speeches

`MigParl` is a thematic subcorpus of all debates in the German regional state parliaments (Bundesländer). Therefore, a procedure to classify speeches as relevant for this subcorpus is needed. Here, we follow a two pronged approach:

* The first central approach to this endeavour is the deployment of a topic modelling approach in which we classify speeches by their association with a computed topic.
* The second method used is a dictionary approach in which we emulate the procedure used for the creation of the `MigPress` corpus of newspaper articles. 

## Topic Modelling

Topic Modelling is an unsupervised machine learning approach which is often used for cases like this: We want to classify documents of a collection, separating them into identifiable classes using the textual features of the documents and their manifestations in the collection. This data-driven approach is particularly useful in cases in which we are not sure which topics to expect in a corpus [@Silge.2019, chapter 6]. The technical implementation of the topic modelling procedure is explained elsewhere. Importantly, we use the Latent Dirichlet allocation (LDA) as the approach to fit the topic models. With LDA we can estimate the topic probability for each document [@Silge.2019, chapter 6]. Before fitting the model, we removed short documents (> 100 words) as well as noise (i.e. stop words, short words (shorter than three characters) and numbers). After stop word removal, we remove remaining rare words which occur less than 11 times and finally documents which are empty after the preprocessing. We fit the document with a k of 250 which proved to be reasonable for parliamentary data. Alpha is 0.2 in these cases.

## Topic Selection

In the beta version of the `MigParl` corpus (build date 2018-11-27) we calculated a topic model for each regional parliament corpus and identified the relevant topics qualitatively by checking the 50 most relevant terms per topic by hand. 

In this update, we still perform a topic model for each regional parliament. However, instead of evaluating 4000 topics manually, we used a predefined list of key terms to determine their relevance for the selection. The initial list was created by @Blatte.2017 comprising about 800 terms which were collected in a semi-supervised fashion, starting from a number of seed words and expanding on them by exploring composita which actually occur in the data. 

### Dictionary

Using a list, we are confronted with a trade-off between precision (using a sparse amount of keywords to grasp a well-defined group of speeches) and a broad selection, consisting of more documents than we would want in a narrow definition. Diverging from the initial list by @Blatte.2017, we decided to use a broader definition, accepting false positives as we wanted to retain the possibility to filter the classification results afterwards. After labelling two topic models manually to establish a standard understanding of the relevance of a topic, we extended the list in an iterative process until the dictionary approach matched the human classification without introducing too many non-topical positives. To this end, we included a number of terms which are not necessarily referring exclusively to migration and integration and which were in part omitted from the initial list because of their ambiguity: "Leitkultur", "Identität" and "Integration" describing cultural aspects, "Terror-, "Anschlag", "Anschläge", "Gefährder" and "Sicherheit-" as indicators of security discourse and "Ausland", "ausländ-" and "Europa" to cover references to international contexts. Again, these terms can be characterised by a high degree of ambiguity and should not suggest a certain understanding of integration and migration. On the contrary, the inclusion these terms enables us to uncover the connection between migration and integration discourse on the one hand and adjacent topoi. This is something that would not be possible with a narrower dictionary.

Some words of the list were shortened by a hyphen, such as "Schmuggler-" (facilitators of (illegal) migration) which for example might also include "Schmugglerbande". This possibility was implemented as well.

The resulting list is as dictionary:

```{r 01_preparing_the_dictionary_for_tm, echo = FALSE, eval = TRUE}
library(magrittr)

# reading list from .txt.
BlaetteWuest <- readLines("./dictionary.txt")
  

added_terms <- c("Leitkultur", "Identität", "Integration", "Terror-", "Anschlag", "Anschläge", "Gefährder", "Sicherheit-", "Ausland", "ausländ-","Europa")

# didn't include "europäisch-" as those terms tend to stick together to describe an EU topic.

mig_int_dict <- sort(c(BlaetteWuest, added_terms)) %>% 
  paste(collapse = "$|^") %>%
  gsub("-\\$\\|", ".*?$|", .) %>%
  gsub("\\^\\-", "\\^.*?", .) %>%
  paste0("^", ., "$")

DT::datatable(as.matrix(unlist(strsplit(mig_int_dict, "\\|"))), colnames = "Term", 
              options = list(scrollY = TRUE))
```


### Semi-Automatic Labelling

We test the approach with the initial list proposed by @Blatte.2017 extended by the words described above. As the central evaluation step, we want to compare the 100 most relevant words of each topic we modelled with this list. We will label the topic as relevant if at least five words of the list also occur in the 100 most relevant words of the topic. 

```{r 01_identify_relevant_topics_by_list, eval = TRUE, echo = FALSE}

# loading the term-topic-matrix of the lda topic model for each regional state

list_of_topicterms <- lapply(list.files(system.file(package = "PopParl", "extdata", "supplementary_data", "topicmodels"), full.names = TRUE),
       function(x) {
         reg_state_name <- substr(basename(x), 5,6)
         lda_model <- readRDS(x)
         lda_model_terms <- topicmodels::terms(lda_model, 100)
         colnames(lda_model_terms) <- paste(reg_state_name, colnames(lda_model_terms), sep = "_")
         lda_model_terms <- t(lda_model_terms)
         return(lda_model_terms)
       }
)

# for each matrix, create a count for each row of hits for relevant terms and
# collect topics with at least 5 hits (this takes a while because of the huge
# keyword list we use for grepping)

relevant_topic_char.list <- unlist(lapply(list_of_topicterms, 
                                   function(xx) {
                                     state_name <- gsub("(.*?)_.*", "\\1", rownames(xx)[1])
                                     rows <- which(rowSums(`dim<-`(grepl(mig_int_dict, xx, fixed = FALSE), dim(xx))) >= 5) 
                                     interesting_topics <- paste0(rows, collapse = "|")
                                     names(interesting_topics) <- state_name
                                     return(interesting_topics)
                                   }
))


rm(list_of_topicterms)
```

Performed with the initial keyword list, we found `r length(unlist((stringr::str_split(paste0(unname(unlist(relevant_topic_char.list)), collapse = "|"), "\\|"))))` relevant topics. After checking for a last time if these topics seem reasonable we are content that this method improves reproducibility and transparency.

```{r 01_print_and_store_topics, eval = TRUE, echo = FALSE}
all_rel_topics <- lapply(list.files(system.file(package = "PopParl", "extdata", "supplementary_data", "topicmodels"), full.names = TRUE),
       function(x) {
         reg_state_name <- substr(basename(x), 5,6)
         lda_model <- readRDS(x)
         lda_model_terms <- topicmodels::terms(lda_model, 100)
         lda_model_terms <- as.data.frame(lda_model_terms)
         
         topics_for_lda <- relevant_topic_char.list[names(relevant_topic_char.list) == reg_state_name]
         topics_for_lda <- gsub("\\|", "$\\|^", topics_for_lda)
         topics_for_lda <- paste0("^", topics_for_lda, "$")
         colnames(lda_model_terms) <- gsub(".*?\\s(\\d+)$", "\\1", colnames(lda_model_terms))
         lda_model_terms <- lda_model_terms[,grepl(topics_for_lda, colnames(lda_model_terms))]
         colnames(lda_model_terms) <- paste(reg_state_name, colnames(lda_model_terms), sep = "_")
         
         return(lda_model_terms)
       }
)

# print as data.table
do.call("cbind", all_rel_topics) %>%
  DT::datatable(options = list(
     dom = 't',
     scrollX = TRUE,
     scrollCollapse = TRUE,
     pageLength = 15
   ))

# save as named vector
allx <- lapply(1:length(all_rel_topics), 
       function(i) {
         state_name <- strsplit(names(all_rel_topics[[i]]), "_")[[1]][1]
         state_topics <- as.numeric(grep("\\d", unlist(strsplit(names(all_rel_topics[[i]]), "_")), value = TRUE))
         names(state_topics) <- rep(state_name, length(state_topics))
         return(state_topics)
       }
)

unlist_all_state_topics <- unlist(allx)

saveRDS(unlist_all_state_topics, "./rds/all_state_topics.rds")
```